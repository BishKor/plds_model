{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import scipy.sparse as scsp\n",
    "import numpy as np\n",
    "from newton_method import nr_algo\n",
    "\n",
    "\n",
    "def kd(i, j):\n",
    "    if i == j:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bb28895adc6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/BisharaKorkor/.virtualenvs/tf3/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BisharaKorkor/.virtualenvs/tf3/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "np.linalg.inv(np.array([[1,1], [1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logposterior(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd):\n",
    "    \"\"\"\n",
    "    :param y: neuron capture data\n",
    "    :param C: latent space to neuron space transformation matrix\n",
    "    :param d: mean firing rates\n",
    "    :param A: deterministic component of the evolution state[t] to state[t+1]\n",
    "    :param q: covariance of the innovations that perturb the latent state at each time step\n",
    "    :param q0: covariance of the initial state x1 of each trial\n",
    "    :param B: mapping of stimuli to \"latent space stimuli\"\n",
    "    :param u: stimuli (4-D one-hot)\n",
    "    :param nts: number of time steps\n",
    "    :param n_neurons: number of neurons\n",
    "    :return: the log-posterior of eq.4 in Macke et al. 2011\n",
    "    \"\"\"\n",
    "\n",
    "    def logpost(x):\n",
    "\n",
    "        # first compute useful values\n",
    "        q0inv = np.linalg.inv(q0)\n",
    "        qinv = np.linalg.inv(q)\n",
    "\n",
    "        constants = - .5 * np.log(np.linalg.det(q0)) - .5 * (nts-1) * np.log(np.linalg.det(q))\n",
    "        term1a = sum(y[t*n_neurons:(t+1)*n_neurons].T @ (C @ x[t*nld:(t+1)*nld] + d) for t in range(nts))\n",
    "        # print(\"example = {}\".format(np.exp(C @ x[4*nld:(4+1)*nld] + d)))\n",
    "        term1b = - sum(np.sum(np.exp(C @ x[t*nld:(t+1)*nld] + d)) for t in range(nts))\n",
    "\n",
    "        term2 = - .5 * (x[0*nld:(0+1)*nld] - x0).T @ q0inv @ (x[0*nld:(0+1)*nld] - x0)\n",
    "\n",
    "        term3 = -.5 * sum((x[(t+1)*nld:(t+2)*nld] - A @ x[t*nld:(t+1)*nld] -\n",
    "                B @ u[t*nsd:(t+1)*nsd]).T @ qinv @ (x[(t+1)*nld:(t+2)*nld] -\n",
    "                A @ x[t*nld:(t+1)*nld] - B @ u[t*nsd:(t+1)*nsd])\n",
    "                for t in range(nts-1))\n",
    "\n",
    "        return constants + term1a + term1b + term2 + term3\n",
    "\n",
    "    return logpost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg as splin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-60237.299518442203"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp = logposterior(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd)\n",
    "lpd = logposteriorderivative(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd, nld)\n",
    "lph = logposteriorhessian(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd, nld)\n",
    "newton_dir = splin.spsolve(lph(mu), -lpd(mu))\n",
    "lp(mu - .0001 * newton_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.89262359712 7172.21626986\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(mu), np.linalg.norm(newton_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logposteriorderivative(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd, nld):\n",
    "    def f(x):\n",
    "        df = np.empty_like(x)\n",
    "        Qinv = np.linalg.inv(q)\n",
    "        Q0inv = np.linalg.inv(q0)\n",
    "        ATQinv = A.T @ Qinv\n",
    "        ATQinvA = A.T @ Qinv @ A\n",
    "        ATQinvB = A.T @ Qinv @ B\n",
    "        QinvA = Qinv @ A\n",
    "        QinvB = Qinv @ B\n",
    "        df[0*nld:(0+1)*nld] = -Q0inv @ (x[0*nld:(0+1)*nld] - x0) + \\\n",
    "                              ATQinv @ (x[1*nld:(1+1)*nld] - A @ x[0*nld:(0+1)*nld] - B @ u[0*nsd:1*nsd])\n",
    "\n",
    "        for t in range(1, nts-1):\n",
    "            df[t*nld:(t+1)*nld] = C.T @ y[t*n_neurons:(t+1)*n_neurons] \\\n",
    "                                  - C.T @ np.exp(C @ x[t*nld:(t+1)*nld] + d) \\\n",
    "                                  + ATQinv @ x[(t+1)*nld:(t+2)*nld] \\\n",
    "                                  - ATQinvA @ x[t*nld:(t+1)*nld] \\\n",
    "                                  - ATQinvB @ u[t*nsd:(t+1)*nsd] \\\n",
    "                                  - Qinv @ x[t*nld:(t+1)*nld] \\\n",
    "                                  + QinvA @ x[(t-1)*nld:t*nld] \\\n",
    "                                  + QinvB @ u[(t-1)*nsd:t*nsd]\n",
    "\n",
    "        df[(nts-1)*nld:nts*nld] = C.T @ y[(nts-1)*n_neurons:nts*n_neurons] - C.T @ np.exp(C @ x[(nts-1)*nld:nts*nld] + d) \\\n",
    "                                  - Q0inv @ (x[(nts-1)*nld:nts*nld] + A @ x[(nts-2)*nld:(nts-1)*nld] + B @ u[(nts-2)*nsd:(nts-1)*nsd])\n",
    "        return df\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logposteriorhessian(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd, nld):\n",
    "    def f(x):\n",
    "        Qinv = np.linalg.inv(q)\n",
    "        Q0inv = np.linalg.inv(q0)\n",
    "        ATQinvA = A.T @ Qinv @ A\n",
    "        ATQinv = - A.T @ Qinv\n",
    "        ATQinvAminusQinv = - ATQinvA - Qinv\n",
    "\n",
    "        diag = []\n",
    "        off_diag = []\n",
    "        diag.append(scsp.lil_matrix(- ATQinvA - Q0inv))\n",
    "        for t in range(nts-2):\n",
    "            diag.append(scsp.lil_matrix(-sum(np.exp(C[i]*x[t*nld:(t+1)*nld] + d[i]) * np.outer(C[i], C[i].T)\n",
    "                                          for i in range(n_neurons)) + ATQinvAminusQinv))\n",
    "        diag.append(scsp.lil_matrix(-Qinv))\n",
    "\n",
    "        for t in range(0, nts):\n",
    "            off_diag.append(scsp.lil_matrix(.5 * ATQinv))\n",
    "\n",
    "        h = scsp.block_diag(diag).tolil()\n",
    "        od = scsp.block_diag(off_diag).tolil()\n",
    "        h[nld:, :] += od[:-nld, :]\n",
    "        h[:, nld:] += od.T[:, :-nld]\n",
    "        return h.tocsc()\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jointloglikelihood(y, nsd, n_neurons, nts, mu, cov, Q, Q0, x0, A, u, B):\n",
    "    def jointll(dC):\n",
    "\n",
    "        d = np.empty(n_neurons)\n",
    "        C = np.empty((n_neurons, nld))\n",
    "\n",
    "        for i in range(n_neurons+1):\n",
    "            d[i] = dC[i*(nld+1)]\n",
    "            C[i] = dC[i*(nld+1) + 1:(i+1)*(nld+1)]\n",
    "\n",
    "        # precompute for efficiency\n",
    "        q0inv = np.linalg.inv(Q0)\n",
    "        qinv = np.linalg.inv(Q)\n",
    "\n",
    "        jll = sum(y[t*n_neurons:(t+1)*n_neurons].T @ C @ mu[t*nld:(t+1)*nld] + y[t*n_neurons:(t+1)*n_neurons].T @ d \\\n",
    "                  - .5 * np.exp(C @ mu[t*nld:(t+1)*nld] +\n",
    "                    .5 * C.T @ cov[t*n_neurons:(t+1)*n_neurons, t*n_neurons:(t+1)*n_neurons] @ C + d) \\\n",
    "                   - .5 * mu[nld:2*nld].T @ q0inv @ mu[nld:2*nld] \\\n",
    "                   + .5 * np.trace(q0inv @ cov[nld:2*nld, 1*nld:2*nld]) \\\n",
    "                   + .5 * mu[nld:2*nld].T @ q0inv @ x0 \\\n",
    "                   + .5 * x0.T @ q0inv @ mu[nld:2*nld] \\\n",
    "                   - .5 * x0.T @ q0inv @ mu[nld:2*nld] \\\n",
    "                   - .5 * x0.T @ q0inv @ x0 \\\n",
    "                   - .5 * mu[(t+1)*nld:(t+2)*nld] @ q0inv @ mu[(t+1)*nld:(t+2)*nld] \\\n",
    "                   + np.trace(qinv @ cov[(t+1)*nld, (t+1)*nld]) \\\n",
    "                   + .5 * mu[(t+1)*nld:(t+2)*nld].T * qinv @ A @ mu[t*nld:(t+1)*nld] \\\n",
    "                   + np.trace(qinv @ A @ cov[t*nld:(t+1)*nld, (t+1)*nld:(t+2)*nld]) \\\n",
    "                   + .5 * mu[(t+1)*nld].T @ qinv @ B @ u[t*nsd:(t+1)*nsd] \\\n",
    "                   + .5 * mu[t*nld:(t+1)*nld].T @ A.T @ qinv @ mu[(t+1)*nld:(t+2)*nld] \\\n",
    "                   + np.trace(A @ qinv @ cov[(t+1)*nld:(t+2)*nld, t*nld:(t+1)*nld]) \\\n",
    "                   - .5 * mu[t*nld:(t+1)*nld].T @ A.T @ qinv @ A @ B @ u[t*nsd:(t+1)*nsd] \\\n",
    "                   + np.trace(A.T @ qinv @ A @ cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld]) \\\n",
    "                   - .5 * mu[t*nld:(t+1)*nld].T @ A.T @ qinv @ B @ u[t*nsd:(t+1)*nsd] \\\n",
    "                   + .5 * u[t*nsd:(t+1)*nsd].T @ B.T @ qinv @ mu[(t+1)*nld:(t+2)*nld] \\\n",
    "                   - .5 * u[t*nsd:(t+1)*nsd].T @ B.T @ qinv @ A @ mu[(t+1)*nld:(t+2)*nld] \\\n",
    "                   - .5 * u[t*nsd:(t+1)*nsd].T @ B.T @ qinv @ B @ u[t*nsd:(t+1)*nsd] for t in range(nts-1)) \\\n",
    "              - .5 * np.log(np.linalg.det(Q0)) - .5 * (nts-1) * np.log(np.linalg.det(Q))\n",
    "        return jll\n",
    "    return jointll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jllDerivative(n_neurons, nld, mu, cov, nts, y):\n",
    "    def f(dC):\n",
    "        d = np.empty(n_neurons)\n",
    "        C = np.empty((n_neurons, nld))\n",
    "\n",
    "        for i in range(n_neurons+1):\n",
    "            d[i] = dC[i*(nld+1)]\n",
    "            C[i] = dC[i*(nld+1) + 1:i*(nld+1)]\n",
    "\n",
    "        djlld = sum(\n",
    "                y[t] + np.exp(C @ mu[t*nld:(t+1)*nld] + d + .5 * np.diag(C.T @ cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C))\n",
    "                for t in range(nts))\n",
    "\n",
    "        djllC = np.array([\n",
    "                sum(\n",
    "                y[t][i] * cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C[i] + mu[t] +\n",
    "                np.exp(C[i] @ mu[t*nld:(t+1)*nld] + d[i] + .5 * C[i] @ cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C[i]) *\n",
    "                (u[t] + cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C[i])\n",
    "                for t in range(nts))\n",
    "                for i in range(n_neurons)])\n",
    "\n",
    "        djlldC = np.array([np.concatenate(djlld[i], djllC[i]) for i in range(n_neurons)])\n",
    "        return djlldC\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jllHessianOptimized(n_neurons, nld, mu, cov, nts, y):\n",
    "    def f(dC):\n",
    "        d = np.empty(n_neurons)\n",
    "        C = np.empty((n_neurons, nld))\n",
    "\n",
    "        for i in range(n_neurons+1):\n",
    "            d[i] = dC[i*(nld+1)]\n",
    "            C[i] = dC[i*(nld+1) + 1:i*(nld+1)]\n",
    "\n",
    "        blocks = []\n",
    "        block = np.zeros((1 + nld)*(1 + nld)).reshape(-1, 1+nld)\n",
    "        \n",
    "        for i in range(n_neurons):\n",
    "            block[0, 0] = sum(np.exp(C[i] @ mu[t*nld:(t+1)*nld] + d[i] +\n",
    "                                                    .5 * C[i] @ cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C[i])\n",
    "                                                    for t in range(nts))\n",
    "\n",
    "            block[1, 1:] = sum((mu[t] + cov[t, t] @ C[i]) * np.exp(C[i] @ mu[t*nld:(t+1)*nld] + d[i] +\n",
    "                    .5 * C[i] @ cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C[i]) for t in range(nts))\n",
    "\n",
    "            block[1:, 1] = block[1, 1:].T\n",
    "\n",
    "            block[1:, 1:] = np.array([sum(y[t][i] * cov[t, t] @ C[i] + mu[t] +\n",
    "                np.outer(mu[t] + cov[t, t] @ C[i], mu[t] + cov[t, t] @ C[i])[i] +\n",
    "                np.exp(C[i] @ mu[t*nld:(t+1)*nld] + d[i] + .5 * C[i] @ cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C[i]) *\n",
    "                (u[t] + cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] @ C[i])\n",
    "                for t in range(nts))\n",
    "                ])\n",
    "        HJLL = scsp.block_diag(blocks)\n",
    "        return HJLL\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def laplace_approximation(f, df, hf, x):\n",
    "    # use NR algorithm to compute minimum of log-likelihood\n",
    "    x = nr_algo(f, df, hf, x)\n",
    "    # negative inverse of Hessian is covariance matrix\n",
    "    covariance = -scsp.linalg.inv(hf(x)).toarray()\n",
    "    return mu, covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "variable initialization\n",
      "begin training\n",
      "epoch 0\n",
      "performing laplace approximation\n",
      "nr iteration: 0\n",
      "computing newton direction\n",
      "fnew - fold = -15695.646199850213\n",
      "newtonstep magnitude = 118.3617112379308\n",
      "returning updated locations\n",
      "final function value in nr_algo = -3021.7896323985774\n",
      "assigning analytic expressions\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-0d131c2caf77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         B = sum(np.outer(mu[(t+1)*nld:(t+2)*nld], u[t*nsd:(t+1)*nsd].T) - A @ np.outer(mu[t*nld:(t+1)*nld], u[t*nsd:(t+1)*nsd])\n\u001b[0;32m---> 71\u001b[0;31m                 for t in range(nts-1)) @ np.linalg.inv(sum(np.outer(u[t*nsd:(t+1)*nsd], u[t*nsd:(t+1)*nsd])for t in range(nts-1)))\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'creating instance of joint log likelihood'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BisharaKorkor/.virtualenvs/tf3/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BisharaKorkor/.virtualenvs/tf3/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    print('loading data')\n",
    "    nts = 500\n",
    "    n_neurons = 300  # number of neurons\n",
    "    nld = 5  # number of latent dimensions\n",
    "    nsd = 4\n",
    "    frameHz = 10  # frames per seconds\n",
    "    data = scio.loadmat('data/compiled_dF033016.mat')\n",
    "    y = data['behavdF'].flatten()\n",
    "    onset = np.array(data['onsetFrame'].T[0], np.int8)\n",
    "    resptime = data['resptime'].T[0]\n",
    "    correct = data['correct'][0]\n",
    "    orient = np.array(data['orient'][0], np.int8)\n",
    "    location = np.array((data['location'][0]+1)//2, np.int8)\n",
    "    u = np.zeros((nts, nsd))\n",
    "    for ot, rt, cor, ori, loc in zip(onset, resptime, correct, orient, location):\n",
    "        # compute what u should be here\n",
    "        u[int(ot):ot+int((rt+2.75+(4.85-2.75)*(1-cor))*frameHz)] = np.array([ori*loc, (1-ori)*loc, ori*(1-loc), (1-ori)*(1-loc)], np.int)\n",
    "    u = u.flatten()\n",
    "\n",
    "    print('variable initialization')\n",
    "    # Initialize parameters to random values\n",
    "    C = np.random.randn(n_neurons, nld)/1000\n",
    "    d = np.random.randn(n_neurons)/1000\n",
    "    x0 = np.random.randn(nld)/1000\n",
    "    A = np.random.rand(nld, nld)\n",
    "    q0 = np.random.rand(nld, nld)\n",
    "    q0 = np.dot(q0.T, q0)\n",
    "    q = np.random.rand(nld, nld)\n",
    "    q = np.dot(q, q.T)\n",
    "    B = np.random.rand(nld, nsd)\n",
    "    mu = np.random.rand(nld*nts)\n",
    "    cov = np.random.rand(nld, nld)\n",
    "\n",
    "    print('begin training')\n",
    "    max_epochs = 2\n",
    "    for epoch in range(max_epochs):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        print('performing laplace approximation')\n",
    "        # perform laplace approximation on log-posterior with Newton-Raphson optimization to find mean and covariance\n",
    "        mu, cov = laplace_approximation(logposterior(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd),\n",
    "                                        logposteriorderivative(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd, nld),\n",
    "                                        logposteriorhessian(y, C, d, A, B, q, q0, x0, u, nts, n_neurons, nsd, nld),\n",
    "                                        mu)\n",
    "\n",
    "        print('assigning analytic expressions')\n",
    "        # Use analytic expressions to compute parameters x0, Q, Q0, A, B\n",
    "        x0 = mu[0:nld]\n",
    "        q0 = cov[0:nld, 0:nld]\n",
    "\n",
    "        A = sum(cov[(t+1)*nld:(t+2)*nld, t*nld:(t+1)*nld] + np.outer(mu[(t+1)*nld:(t+2)*nld], mu[t*nld:(t+1)*nld].T) for t in range(nts - 1)) @ \\\n",
    "            np.linalg.inv(sum(cov[t*nld:(t+1)*nld, t*nld:(t+1)*nld] +\n",
    "            np.outer(mu[t*nld:(t+1)*nld], mu[t*nld:(t+1)*nld].T) for t in range(nts - 1)))\n",
    "\n",
    "        q = sum(cov[(t + 1)*nld:(t + 2)*nld, (t + 1)*nld:(t + 2)*nld] +\n",
    "                np.outer(mu[(t + 1)*nld:(t + 2)*nld], mu[(t + 1)*nld:(t + 2)*nld].T) -\n",
    "                (cov[(t + 1)*nld:(t + 2)*nld, t*nld:(t + 1)*nld] +\n",
    "                np.outer(mu[(t + 1)*nld:(t + 2)*nld], mu[t*nld:(t + 1)*nld])) @ A.T -\n",
    "                np.outer(mu[(t + 1)*nld:(t + 2)*nld], u[t*nsd:(t+1)*nsd].T) @ B.T +\n",
    "                A @ (cov[t*nld:(t + 1)*nld, (t + 1)*nld:(t + 2)*nld] +\n",
    "                mu[t*nld:(t+1)*nld] @ mu[(t + 1)*nld:(t+2)*nld].T) +\n",
    "                A @ (cov[t*nld:(t + 1)*nld, t*nld:(t + 1)*nld] +\n",
    "                np.outer(mu[t*nld:(t+1)*nld], mu[t*nld].T)) @ A.T +\n",
    "                A @ np.outer(mu[t*nld:(t+1)*nld], u[t*nsd:(t+1)*nsd].T) @ B.T -\n",
    "                B @ np.outer(u[t*nsd:(t+1)*nsd], mu[t*nld:(t+1)*nld].T) +\n",
    "                B @ np.outer(u[t*nsd:(t+1)*nsd], mu[t*nld:(t+1)*nld].T) @ A.T +\n",
    "                B @ np.outer(u[t*nsd:(t+1)*nsd], u[t*nsd:(t+1)*nsd]) @ B.T\n",
    "                for t in range(nts - 1))\n",
    "\n",
    "        B = sum(np.outer(mu[(t+1)*nld:(t+2)*nld], u[t*nsd:(t+1)*nsd].T) - A @ np.outer(mu[t*nld:(t+1)*nld], u[t*nsd:(t+1)*nsd])\n",
    "                for t in range(nts-1)) @ np.linalg.inv(sum(np.outer(u[t*nsd:(t+1)*nsd], u[t*nsd:(t+1)*nsd])for t in range(nts-1)))\n",
    "\n",
    "        print('creating instance of joint log likelihood')\n",
    "        # Create instance of joint log posterior with determined parameters\n",
    "        jll = jointloglikelihood(y, nsd, n_neurons, nts, mu, cov, q, q0, x0, A, u, B)\n",
    "\n",
    "        # Second NR minimization to compute C, d (and in principle, D)\n",
    "\n",
    "        print('performing NR algorithm for parameters C, d')\n",
    "        # need to vectorize C for the purpose of gradient descent, thus making a vector (d[i], C[i]), i.e. hessian for\n",
    "        # each neuron\n",
    "        dC = np.array([np.concatenate(d[i], C[i]) for i in range(n_neurons)])\n",
    "\n",
    "        dC = nr_algo(jll, jllDerivative(n_neurons, nld, mu, cov, nts, y),\n",
    "                     jllHessianOptimized(n_neurons, nld, mu, cov, nts, y), dC)\n",
    "\n",
    "        for i in range(n_neurons+1):\n",
    "            d[i] = dC[i*(nld+1)]\n",
    "            C[i] = dC[i*(nld+1) + 1:i*(nld+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable initialization\n"
     ]
    }
   ],
   "source": [
    "nts = 100\n",
    "n_neurons = 300  # number of neurons\n",
    "nld = 5  # number of latent dimensions\n",
    "nsd = 4\n",
    "frameHz = 10  # frames per seconds\n",
    "data = scio.loadmat('data/compiled_dF033016.mat')\n",
    "y = data['behavdF'].flatten()\n",
    "onset = np.array(data['onsetFrame'].T[0], np.int8)\n",
    "resptime = data['resptime'].T[0]\n",
    "correct = data['correct'][0]\n",
    "orient = np.array(data['orient'][0], np.int8)\n",
    "location = np.array((data['location'][0]+1)//2, np.int8)\n",
    "u = np.zeros((nts, nsd))\n",
    "for ot, rt, cor, ori, loc in zip(onset, resptime, correct, orient, location):\n",
    "    # compute what u should be here\n",
    "    u[int(ot):ot+int((rt+2.75+(4.85-2.75)*(1-cor))*frameHz)] = np.array([ori*loc, (1-ori)*loc, ori*(1-loc), (1-ori)*(1-loc)], np.int)\n",
    "u = u.flatten()\n",
    "\n",
    "print('variable initialization')\n",
    "# Initialize parameters to random values\n",
    "C = np.random.randn(n_neurons, nld)/1000\n",
    "d = np.random.randn(n_neurons)/1000\n",
    "x0 = np.random.randn(nld)/1000\n",
    "A = np.random.rand(nld, nld)\n",
    "q0 = np.random.rand(nld, nld)\n",
    "q0 = np.dot(q0.T, q0)\n",
    "q = np.random.rand(nld, nld)\n",
    "q = np.dot(q, q.T)\n",
    "B = np.random.rand(nld, nsd)\n",
    "mu = np.random.rand(nld*nts)\n",
    "cov = np.random.rand(nld, nld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.,   0.,   0.,   0.],\n",
       "       [  0.,   2.,   0.,   0.],\n",
       "       [  0.,   0.,  67.,   0.],\n",
       "       [  0.,   0.,   0.,  17.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.outer(u[t*nsd:(t+1)*nsd], u[t*nsd:(t+1)*nsd]) for t in range(nts-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
